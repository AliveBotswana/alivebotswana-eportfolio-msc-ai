# The Jaccard Coefficient: 

# DATASET: 
The following table presents the pathological test results for three individuals. The attributes include Name, Gender, and various test results (Fever, Cough, Test-1, Test-2, Test-3, Test-4).

Name,Gender,Fever,Cough,Test-1,Test-2,Test-3,Test-4
Jack,M,Y,N,P,N,N,A
Mary,F,Y,N,P,A,P,N
Jim,M,Y,P,N,N,N,A

# Task: Jaccard Coefficient calculation:
Calculate the Jaccard coefficient (similarity measure) for the following pairs, focusing on the asymmetric binary attributes (Fever, Cough, Test-1, Test-2, Test-3, Test-4). Convert Y/P to 1 (presence/abnormal) and N/A to 0 (absence/normal). 
(Jack, Mary)
(Jack, Jim)
(Jim, Mary)

# Formula Reminder:
Jaccard Similarity = $\frac{f_{11}}{f_{11} + f_{10} + f_{01}}$, where:

$f_{11}$: Number of attributes where both are 1.
$f_{10}$: Number where first is 1 and second is 0.
$f_{01}$: Number where first is 0 and second is 1.
(Ignore $f_{00}$ as it's not meaningful for asymmetric data.)

# ANSWER: 
Individual,Fever,Cough,Test-1,Test-2,Test-3,Test-4
Jack,1,0,1,0,0,0
Mary,1,0,1,0,1,0
Jim,1,1,0,0,0,0

Pairwise results:

(Jack, Mary): $ f_{11} = 2 $ (Fever, Test-1), $ f_{10} = 0 $, $ f_{01} = 1 $ (Test-3).
            Jaccard = 2 / (2 + 0 + 1) = 2/3 ≈ 0.6667

(Jack, Jim):$ f_{11} = 1 $ (Fever), $ f_{10} = 1 $ (Test-1), $ f_{01} = 1 $ (Cough).
            Jaccard = 1 / (1 + 1 + 1) = 1/3 ≈ 0.3333
(Jim, Mary):$ f_{11} = 1 $ (Fever), $ f_{10} = 1 $ (Cough), $ f_{01} = 2 $ (Test-1, Test-3).
            Jaccard = 1 / (1 + 1 + 2) = 1/4 = 0.25

# DISCUSSION: 
Articulate the Legal, Social, Ethical, and Professional Issues Faced by Machine Learning ProfessionalsMachine learning (ML) professionals encounter multifaceted 
issues across legal, social, ethical, and professional domains, especially when handling sensitive data like health records in this pathological dataset.

Legal Issues: Machine Learning is a discipline heavily reliant on data handling and processing. Machine Learning applications must comply with data protection 
regulations, such as the General Data Protection Regulation (GDPR) in the EU or the Health Insurance Portability and Accountability Act (HIPAA) in the US, which 
classify health data as sensitive and mandate explicit consent, anonymization, and secure processing to prevent breaches (Said, Yahyaoui and Abdellatif, 2024). 
These regulations provide a more objective framework that ensures the introduction and use of AI/ML minimizes risks of bias or error, protects patient well-being, 
safeguards privacy, and promotes accountability (Drabiak et al., 2023). Liability arises if ML models cause harm, such as misdiagnosis, raising questions about 
accountability for algorithmic decisions. Privacy laws challenge AI use, as models trained on personal data risk re-identification, even in aggregated forms
like similarity coefficients. In hiring or healthcare, legal risks include discrimination claims if biased data violates equality laws.

Reference
Drabiak, K., Kyzer, S., Nemov, V. and El Naqa, I. (2023) 'AI and machine learning ethics, law, diversity, and global impact', British Journal of Radiology, 96(1150), 
article number 20220934. doi:10.1259/bjr.20220934.

Said, A., Yahyaoui, A. and Abdellatif, T. (2024) 'HIPAA and GDPR Compliance in IoT Healthcare Systems'. In: Advances in Model and Data Engineering in the Digitalization 
Era (MEDI 2023). Cham: Springer International Publishing, pp. 198-209.
